# Navier-Stokes 求解器优化任务清单

## 项目背景

当前有一个**基于MPI+OpenMP混合并行的Navier-Stokes方程求解器**，使用FFTW库进行快速傅里叶变换。

## 核心问题

当前代码使用的**FFTW库存在严重的可扩展性限制**：

- **FFTW使用Slab分解(1D)**：只在一个方向切分数据，导致进程数不能超过网格大小N
- 例如：128×128×128的网格，最多只能用128个进程
- 这在现代超算上不够用（如MGU-270单节点就有128核，总共40个节点）

## 任务阶段

### 第一阶段：调研与分析（当前任务）

#### 1. 调研现有分布式FFT库

**需要研究的库：**

- **P3DFFT** (Pekurovsky, 2012)
  - 最知名的Pencil分解库
  - CPU Backend: ESSL / FFTW
  - 支持Slab和Pencil (1D和2D)

- **PFFT** (Pippig & Potts, 2013)
  - P3DFFT的进一步发展
  - 性能与P3DFFT相当
  - 支持高维数组
  - CPU Backend: FFTW

- **AccFFT** (2015)
  - 首个支持计算与通信重叠的库
  - CPU Backend: FFTW
  - GPU Backend: cuFFT

- **fftMPI** (2018)
  - 高效3D FFT计算
  - CPU Backend: FFTW / KISS / MKL

- **heFFTe** (2020)
  - 最通用的库：支持AMD、Intel、NVIDIA GPU
  - 支持高维数组
  - CPU Backend: FFTW / MKL
  - GPU Backend: cuFFT / rocFFT / oneMKL
  - 支持1D/2D/3D分解（slab/pencil/brick）

**重点理解：**
- 各库的工作原理
- Pencil分解 vs Slab分解的区别
- 如何优化通信（非阻塞通信、计算通信重叠）
- Backend库的作用

#### 2. 文献调研

**需要查找：**
- 比较不同FFT库性能的论文
- 在不同架构（特别是类似MGU-270）上的表现
- 已发现的相关论文：Summit超算上的性能对比

**关键发现：**
- AccFFT和heFFTe在Summit上表现最好
- heFFTe的Pencil分解特别高效
- 现代库都优于FFTW MPI和P3DFFT

#### 3. 准备阶段性汇报

**汇报内容：**
- FFT基础知识（什么是FFT，为什么重要）
- 单设备FFT库对比（FFTW、MKL、ESSL等）
- 分布式FFT库介绍
- Slab vs Pencil分解详解
  - Slab：O(N)可扩展性
  - Pencil：O(N²)可扩展性
- 现代库的优化技术
  - 非阻塞通信（MPI_Isend/MPI_Irecv）
  - 计算通信重叠
- 适合MGU-270的候选库推荐

### 第二阶段：代码重构（后续任务）

#### 1. 选择最优库

**评估标准：**
- 性能表现
- 可扩展性
- 易用性
- 文档完善程度
- 社区活跃度

**可能的选择：**
- heFFTe（最推荐，支持GPU，通用性强）
- AccFFT（性能好，但可能文档较少）
- P3DFFT（成熟稳定，但性能可能不是最优）

#### 2. 重写代码

**任务：**
- 将当前基于FFTW的实现改为使用新库
- 需要修改的主要函数：
  - `initialize_fwd_r2c_*` 系列函数
  - `initialize_bwd_c2r_*` 系列函数
  - `initialize_r2r_*` 系列函数
  - FFT执行部分
- 保持算法逻辑不变，只替换FFT部分
- 处理不同的数据分布方式（Pencil vs Slab）

**关键文件：**
- `NavierSrokes_mpiomp_patient.cpp`（行26-92：FFT初始化和执行）

#### 3. 验证正确性

**测试项目：**
- 对比新旧版本的计算结果
- 检查能量守恒
- 验证误差指标（err1, err2, err3）

#### 4. 性能测试

**测试计划：**
- 在MGU-270上测试不同规模
- 测试不同进程数的可扩展性
- 与原FFTW版本对比
- 生成性能报告（时间、加速比、效率）

**测试场景：**
- 小规模：128³（验证正确性）
- 中等规模：256³、512³
- 大规模：1024³（测试极限可扩展性）
- 进程数：1, 4, 16, 64, 256, 1024+

### 第三阶段：GPU扩展（未来计划）

#### 1. 学习前人经验

- 参考Yoann Dumoulan的工作
- 了解CUDA编程基础
- 学习GPU上的FFT实现

#### 2. GPU代码移植

**技术栈：**
- CUDA
- heFFTe（GPU Backend: cuFFT）
- MPI + CUDA通信

**任务：**
- 将计算核心移植到GPU
- 优化GPU内存管理
- 处理CPU-GPU数据传输
- 实现多GPU分布式计算

#### 3. 多节点多GPU

**目标：**
- N个节点，每节点M个GPU
- 使用CUDA-aware MPI或类似技术
- 实现最优的可扩展性

## 关键技术概念

### Slab分解 vs Pencil分解

**Slab分解（1D）：**
- 只在一个方向切分（如沿X轴）
- 每个进程处理一个YZ平面
- 可扩展性：O(N)
- 限制：进程数 ≤ N

**Pencil分解（2D）：**
- 在两个方向切分（如沿X和Y）
- 每个进程处理一根"铅笔"（沿Z轴）
- 可扩展性：O(N²)
- 限制：进程数 ≤ N²

### 通信优化技术

**传统方法（P3DFFT）：**
- 使用阻塞通信：MPI_Alltoallv
- 进程等待通信完成
- 无法重叠计算

**现代方法（AccFFT, heFFTe）：**
- 使用非阻塞通信：MPI_Isend/MPI_Irecv
- 数据打包与通信重叠
- 减少等待时间
- 通信时间占比从60%降低

### Backend库的作用

所有分布式FFT库都需要在每个进程内部执行串行FFT，这部分由backend库完成：

**CPU Backend：**
- FFTW（最优性能，通用）
- MKL（Intel优化）
- ESSL（IBM优化）

**GPU Backend：**
- cuFFT（NVIDIA）
- rocFFT（AMD）
- oneMKL（Intel GPU）

## 预期成果

### 课程作业阶段
- 完整的调研报告
- 各FFT库对比分析
- 技术选型建议

### 硕士论文阶段
- 高性能Navier-Stokes求解器（CPU版）
- 性能测试报告和分析
- 可能的GPU扩展
- 学术论文发表

## 参考资源

### 已知文件
- `NavierSrokes_mpiomp_patient.cpp` - 当前实现
- `我和导师关于课题第一次俄语对话.txt` - 导师指导
- `我的ppt文案.txt` - PPT内容

### 需要查找的资源
- P3DFFT文档和论文
- heFFTe官方文档
- AccFFT论文
- Summit超算性能对比论文
- MGU-270使用手册

## 时间规划

**短期（1-2周）：**
- 完成FFT库调研
- 准备汇报材料

**中期（1-2个月）：**
- 选定库并安装配置
- 重写代码
- 初步测试

**长期（硕士期间）：**
- 全面性能测试
- 撰写论文
- 可能的GPU扩展
